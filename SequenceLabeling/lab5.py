# -*- coding: utf-8 -*-
"""Lab5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ojaSv60YXAKtUUJKcQmUEf-IH8VYy9h

# Link to the lab

https://tinyurl.com/inlplab5

# Setup

We'll use fasttext wiki embeddings in our embedding layer, and pytorch-crf to add a CRF to our BiLSTM.
"""

# !pip install fasttext
# !pip install pytorch-crf
# !pip install datasets
# !pip install sklearn
# !pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

import io
from math import log
import os
from numpy import array
from numpy import argmax
import torch
import random
from math import log
from numpy import array
from numpy import argmax
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch import nn
from torch.optim import AdamW
from torchcrf import CRF
from torch.optim.lr_scheduler import ExponentialLR, CyclicLR
from typing import List, Tuple, AnyStr
from tqdm import tqdm
from sklearn.metrics import precision_recall_fscore_support
import matplotlib.pyplot as plt
from copy import deepcopy
from datasets import load_dataset, load_metric
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import torch.nn.functional as F
import heapq
from transformers import AutoTokenizer
import transformers
from datasets import DatasetDict
from dataclasses import dataclass
import time
import datetime
import sys
from bpemb import BPEmb

Q: Why king is stupid
C: King is stupid because of low cognitive capacity
A: low cognitive capacity

Q: Why king is stupid
C: King is stupid because of low cognitive capacity
A: small cognitive skills



# Q: How to specify class weights for LSTM-CRF? It has a weird loss function?
# Q: For Lab6 can we maybe use a transformer model since we dont get good results here with LSTM? (Karolina: should be fine)
# Q: Model predicts broken-up spans, i.e. "B B I I I O O I I" instead of "B I I I I". How to fix this? (enforce some rules in the forward function)
# Q: Report structure fine? (decrease background, really go into the nitty gritty details about architectures)
# Q: Page limit with and without images or tables? (8 pages with everything)
# Q: Beam search with Bert model. (Karolina: try with BART, not an easy straight forward solution?)
# Q: Code submission, zip file containing each lab's code?

# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip
# !unzip wiki-news-300d-1M.vec.zip

def enforce_reproducibility(seed=42):
    # Sets seed manually for both CPU and CUDA
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # For atomic operations there is currently
    # no simple way to enforce determinism, as
    # the order of parallel operations is not known.
    # CUDNN
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # System based
    random.seed(seed)
    np.random.seed(seed)
enforce_reproducibility()

"""### Define all the necessary classes and utilities
- FasttextTokenizer
- Data loading and parsing
- IOB data preprocessing
- LSTM Model
"""

# constants
BPEmb_EN = BPEmb(lang="en", dim=300, vs=200000)
BPEmb_FI = BPEmb(lang="fi", dim=300, vs=200000)
BPEmb_JA = BPEmb(lang="ja", dim=300, vs=200000)
DEBUG = False
timeStamp = time.strftime("%Y%m%d-%H%M%S")
currFileLoc = os.path.dirname(os.path.realpath(__file__))
logName = os.path.join(currFileLoc, f"l5_log_{timeStamp}.txt")
"""Creates a log file"""
with open(logName, "w") as f:
    f.write("")


def appendToLogFile(text):
    """Appends text to a log file"""
    with open(logName, "a") as f:
        timeStamp = datetime.datetime.now().time()
        f.write(f"{timeStamp}: {text}")
        # Check if text string ends with a new line, if not then add one. Beware of empty text strings.
        if text and text[-1] != "\n":
            f.write("\n")


appendToLogFile("Start of log file \n")
appendToLogFile(f"Using CUDA: {torch.cuda.is_available()} \n")


def printAndLog(text):
    """Prints and logs text"""
    print(text)
    appendToLogFile(text)


class FasttextTokenizer:
    def __init__(self, vocabulary):
        self.vocab = {}
        counter = 0
        for l in vocabulary:
            # add j if l doesnt exist in vocab
            key = l.strip()
            if key not in self.vocab:
                self.vocab[key] = counter
                counter += 1
            else:
                print("duplicate key: ", key)

        self.vocabList = list(self.vocab.keys())
        for key in self.vocab:
            self.vocabList[self.vocab[key]] = key

    def encode(self, text):
        # Text is assumed to be tokenized
        encoded = []
        for t in text:
            if t in self.vocab:
                encoded.append(self.vocab[t])
            else:
                encoded.append(self.vocab["[UNK]"])
        return encoded

    def decode(self, idx):
        return self.vocabList[idx]


@dataclass
class DataPoint:
    """Class that represents a datapoint"""

    qst: str
    qstTokenized: list
    ans: str
    ansTokenized: list  # answer tokenizer
    fullText: str  # raw full text
    fullTextTokenized: str  # full text tokenizer
    prompt: list  # question + full text, tokenized
    lbl: str  # TODO maybe bool instead?


def idxToLabel(idx):
    labels = {0: "O", 1: "B", 2: "I"}
    return labels[idx]


def getLabels():
    return {"O": 0, "B": 1, "I": 2}


def loadTyDiQaDataset():
    return load_dataset("copenlu/answerable_tydiqa")


def calcClassWeights(target):
    """Calculate inverse class weights for a given target"""
    # Collect number of occurences
    target = np.array(target)
    weights = np.bincount(target)
    printAndLog(f"Label counts: O: {weights[0]}, B: {weights[1]}, I: {weights[2]}")

    # Calculate inverse class weights
    weights = 1 / weights
    weights /= weights.sum()
    return weights


def parseData(dataSet: DatasetDict, language: str, dsType: str):
    """Parses the dataset into a list of data structures
    Args:
        dataSet (DatasetDict): Dataset
        language (str): Language for which to get the datapoints
        dsType (str): Type of the dataset (train, validation, test)

    Returns:
        np.ndarray: List of DataPoint entries
    """
    from collections import Counter

    # Prepare utility variables
    cnt = 0
    fails = 0
    data = np.array([], dtype=DataPoint)
    notAnsCnt = 0
    labelCount = []

    # Determine the embeddings vector
    TOKENIZER = None
    if language == "english":
        TOKENIZER = BPEmb_EN
    elif language == "finnish":
        TOKENIZER = BPEmb_FI
    elif language == "japanese":
        TOKENIZER = BPEmb_JA

    # Convert ds to a panda and take out all the goodies
    ds = dataSet[dsType].to_pandas()
    ds = ds.loc[ds["language"] == language]  # Filter out the language
    dsQ = ds["question_text"].values  # Questions
    dsAns = ds["annotations"].values  # Answer stuff
    dsAnsStart = [i["answer_start"][0] for i in dsAns]  # Char idx answer start
    dsAns = [i["answer_text"][0] for i in dsAns]  # Raw shortest answer text
    dsFullText = ds["document_plaintext"].values  # Full text
    assert len(dsQ) == len(dsAns), "Number of questions and answers is not the same"

    # Pack the data in a datastructure and save it to an array
    for i, qst in enumerate(dsQ):
        if not torch.cuda.is_available() and i > 100:
            break

        qst = qst.lower()
        ans = dsAns[i].lower()
        fullText = dsFullText[i].lower()
        ansStart = dsAnsStart[i]

        qstTkn = TOKENIZER.encode(qst)
        ansTkn = TOKENIZER.encode(ans)
        fullTextToken = TOKENIZER.encode(fullText)

        prompt = qstTkn + fullTextToken

        # Find the start and end indices of the answer tokens in the context tokens
        startIdx = -1
        for tknIdx, token in enumerate(prompt):
            if len(ansTkn) == 0:
                notAnsCnt += 1
                break

            if ansTkn[0] == token:
                startIdx = tknIdx  # cover the edge case for single-token ans
                for i in range(1, len(ansTkn)):
                    if ansTkn[i] == prompt[tknIdx + i]:
                        startIdx = tknIdx
                    else:
                        startIdx = -1
                        break
                else:  # Special syntax for successfully finished loops
                    break
        endIdx = startIdx + len(ansTkn) - 1

        # assert correct extraction
        fail = False
        for i, ans in zip(range(startIdx, endIdx + 1), ansTkn):
            if prompt[i] != ans:
                fail = True
                break

        # Log fails for stats
        if fail == True:
            fails += 1
            continue

        # Assign the labels. They should always end up in a continuous sequence
        labels = []
        j = 0
        for i, tkn in enumerate(prompt):
            if i == startIdx:
                if prompt[i] != ansTkn[j]:  # last layer of error-checking
                    print("Error")
                else:
                    labels.append(getLabels()["B"])  # all is OK, append the label
                    j += 1
            elif i > startIdx and i <= endIdx:
                if prompt[i] != ansTkn[j]:  # last layer of error-checking
                    print("Error")
                else:
                    labels.append(getLabels()["I"])  # all is OK, append the label
                    j += 1
            else:
                labels.append(getLabels()["O"])  # if in doubt, just append O

        labelCount = labelCount + labels  # keep track of the categories
        # Q: If a sequence length is too long, skip it?
        if (len(prompt)) > 512:
            cnt += 1
            continue

        # Create the data points
        entry = DataPoint(
            qst, qstTkn, ans, ansTkn, fullText, fullTextToken, prompt, labels
        )
        data = np.append(data, entry)

    printAndLog(f"Unanswerable questions: {notAnsCnt}")
    printAndLog(
        f"Balance of labels: {Counter(labelCount).keys()}:{Counter(labelCount).values()}"
    )
    classWeights = None
    if dsType == "train" and len(sys.argv) > 0:
        classWeights = calcClassWeights(labelCount)
    output = data.ravel()
    printAndLog(f"Len of {dsType} is {len(output)}")
    if cnt > 0 or fail > 0:
        printAndLog(
            f"Skipped {cnt} entries due to too long sequence length (>512; Failed to map answer and to context for {fails}"
        )

    return output, classWeights


def getDataLoader(dataset, bs, shuffle, n_workers=0):
    """Returns a dataloader object"""
    return DataLoader(
        dataset,
        batch_size=bs,
        shuffle=shuffle,
        collate_fn=collate_batch_bilstm,
        num_workers=n_workers,
    )


def extractVocabTokens(trainSet, valSet):
    """Extracts the vocabulary tokens from the dataset"""
    vocabTokens = set()

    # Extract tokens from both sets for full-text, answer and question
    for ds in [trainSet, valSet]:
        for dp in ds:
            for q in dp.qstTokenized:
                vocabTokens.add(q)
            for a in dp.ansTokenized:
                vocabTokens.add(a)
            for ft in dp.fullTextTokenized:
                vocabTokens.add(ft)

    return vocabTokens


def load_vectors(vocabulary):
    """Loads 300x1 word vecs from 'wiki-news-300d-1M.vec'
    Args:
        vocabulary (_type_): Vocabulary to vectorize
    Returns:
        final_vocab, final_vec: final_vocab is vocab with added labels, padding and unkown tokens
                                final_vec is the vectorized vocab
    """
    # Load up the embeddings file
    dirname = os.path.dirname(__file__)
    fname = os.path.join(dirname, "wiki-news-300d-1M.vec")

    with io.open(fname, "r", encoding="utf-8", newline="\n", errors="ignore") as fin:
        # First line is the number of words and the dimensionality of the embeddings
        n, d = map(int, fin.readline().split())

        # Added labels, padding and unknown words
        tag_names = ["O", "B", "I"]
        final_vocab = tag_names + ["[PAD]", "[UNK]", "[BOS]", "[EOS]"]
        final_vectors = [np.random.normal(size=(300,)) for _ in range(len(final_vocab))]
        for j, token in enumerate(vocabulary):
            # if the en, fi, ja token is in the vocabulary, add it to the final_vocab
            # and add the corresponding vector to the final_vectors
            if token in BPEmb_EN.emb:
                final_vocab.append(token)
                final_vectors.append(BPEmb_EN.emb[token])
            elif token in BPEmb_JA.emb:
                final_vocab.append(token)
                final_vectors.append(BPEmb_JA.emb[token])
            elif token in BPEmb_FI.emb:
                final_vocab.append(token)
                final_vectors.append(BPEmb_FI.emb[token])

        final_vec = np.vstack(final_vectors)
        # Compare the vocabulary sizes
        printAndLog(
            f"Vocabulary size: {len(vocabulary)} vs final_vocab {len(final_vocab)}"
        )
        return final_vocab, final_vec


def collate_batch_bilstm(input_data) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Returns a sequence of labels instead of a single label as in text classification."""

    input_ids = [fastTxtTkn.encode(entry.prompt) for entry in input_data]
    seq_lens = [len(i) for i in input_ids]
    labels = [entry.lbl for entry in input_data]

    max_length = max([len(i) for i in input_ids])

    input_ids = [(i + [3] * (max_length - len(i))) for i in input_ids]  # zero-pad
    labels = [
        (i + [0] * (max_length - len(i))) for i in labels
    ]  # 0 is the id of the O tag

    assert all(len(i) == max_length for i in input_ids)
    assert all(len(i) == max_length for i in labels)
    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)


"""### Define basic LSTM model, and training loop"""


class BiLSTM(nn.Module):
    """
    Basic BiLSTM-CRF network for Lab 5
    """

    def __init__(
        self,
        pretrained_embeddings: torch.tensor,
        lstm_dim: int,
        dropout_prob: float = 0.1,
        n_classes: int = 2,
        classWeights=None,
    ):
        """
        Initializer for basic BiLSTM network
        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings
        :param lstm_dim: The dimensionality of the BiLSTM network
        :param dropout_prob: Dropout probability
        :param n_classes: The number of output classes
        """

        # First thing is to call the superclass initializer
        super(BiLSTM, self).__init__()

        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer
        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer
        self.model = nn.ModuleDict(
            {
                "embeddings": nn.Embedding.from_pretrained(
                    pretrained_embeddings, padding_idx=3
                ),
                "bilstm": nn.LSTM(
                    pretrained_embeddings.shape[1],  # input size
                    lstm_dim,  # hidden size
                    2,  # number of layers
                    batch_first=True,
                    dropout=dropout_prob,
                    bidirectional=True,
                ),
                "ff": nn.Linear(2 * lstm_dim, n_classes),
            }
        )
        self.n_classes = n_classes
        appendToLogFile(f"Class Weights {classWeights}")
        if classWeights is not None:
            self.loss = nn.CrossEntropyLoss(weight=torch.FloatTensor(classWeights))
        else:
            self.loss = nn.CrossEntropyLoss()
        # Initialize the weights of the model
        self._init_weights()

    def _init_weights(self):
        all_params = list(self.model["bilstm"].named_parameters()) + list(
            self.model["ff"].named_parameters()
        )
        for n, p in all_params:
            if "weight" in n:
                nn.init.xavier_normal_(p)
            elif "bias" in n:
                nn.init.zeros_(p)

    def forward(self, inputs, input_lens, hidden_states=None, labels=None):
        """
        Defines how tensors flow through the model
        :param inputs: (b x sl) The IDs into the vocabulary of the input samples
        :param input_lens: (b) The length of each input sequence
        :param labels: (b) The label of each sample
        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)
        """

        # Get embeddings (b x sl x edim)
        embeds = self.model["embeddings"](inputs)

        # Pack padded: This is necessary for padded batches input to an RNN - https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch
        lstm_in = nn.utils.rnn.pack_padded_sequence(
            embeds, input_lens.cpu(), batch_first=True, enforce_sorted=False
        )

        # Pass the packed sequence through the BiLSTM
        if hidden_states:
            lstm_out, hidden = self.model["bilstm"](lstm_in, hidden_states)
        else:
            lstm_out, hidden = self.model["bilstm"](lstm_in)

        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)
        lstm_out, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)

        # Get logits (b x seq_len x n_classes)
        logits = self.model["ff"](lstm_out)
        outputs = (logits, lengths)
        if labels is not None:
            loss = self.loss(logits.reshape(-1, self.n_classes), labels.reshape(-1))
            outputs = outputs + (loss,)

        return outputs


"""### Define the training hyperparameters, training and evaluation loops

"""

def evalBasic(model: nn.Module, valid_dl: DataLoader):
    """
    Evaluates the model on the given dataset
    :param model: The model under evaluation
    :param valid_dl: A `DataLoader` reading validation data
    :return: The accuracy of the model on the dataset
    """
    # VERY IMPORTANT: Put your model in "eval" mode -- this disables things like
    # layer normalization and dropout
    model.eval()
    labels_all = []
    preds_all = []
    logPredictionModulus = int(len(valid_dl) / 3)  # log every 4th prediction

    # ALSO IMPORTANT: Don't accumulate gradients during this process
    with torch.no_grad():
        for idx, batch in enumerate(tqdm(valid_dl, desc="Evaluation")):
            batch = tuple(t.to(device) for t in batch)
            input_ids = batch[0]
            seq_lens = batch[1]
            labels = batch[2]
            hidden_states = None

            logits, _, _ = model(
                input_ids, seq_lens, hidden_states=hidden_states, labels=labels
            )
            preds = torch.argmax(logits, dim=-1).reshape(-1).detach().cpu().numpy()
            preds_all.extend(
                torch.argmax(logits, dim=-1).reshape(-1).detach().cpu().numpy()
            )
            labels_all.extend(labels.reshape(-1).detach().cpu().numpy())

            # For logging..
            if idx % logPredictionModulus == 0:
                inputIdsNumpy = input_ids.detach().cpu().numpy()
                labelsNumpy = labels.detach().cpu().numpy()
                predsNumpy = torch.argmax(logits, dim=-1).cpu().numpy()

                dataPointsToLog = [0]
                for seq in dataPointsToLog:
                    prompt = " ".join(
                        [fastTxtTkn.decode(i) for i in inputIdsNumpy[seq] if i != 0]
                    )
                    predAns = ""
                    groundTruth = ""
                    for label in range(labelsNumpy.shape[1]):
                        predLabel = predsNumpy[seq][label]
                        if predLabel == 1 or predLabel == 2:
                            predAns += (
                                fastTxtTkn.decode(inputIdsNumpy[seq][label])
                                + f"({idxToLabel(predLabel)}) "
                            )

                        groundTruthLabel = labelsNumpy[seq][label]
                        if groundTruthLabel == 1 or groundTruthLabel == 2:
                            groundTruth += (
                                fastTxtTkn.decode(inputIdsNumpy[seq][label])
                                + f"({idxToLabel(groundTruthLabel)}) "
                            )
                    appendToLogFile(f"Prmt: {prompt}")
                    appendToLogFile(f"Pred: {predAns}")
                    appendToLogFile(f"GrTh: {groundTruth} \n\n")

    P, R, F1, _ = precision_recall_fscore_support(
        labels_all, preds_all, average="macro"
    )
    print(confusion_matrix(labels_all, preds_all))
    appendToLogFile(np.array2string(confusion_matrix(labels_all, preds_all)))
    printAndLog(classification_report(labels_all, preds_all))
    return F1


def trainBasic(
    model: nn.Module,
    train_dl: DataLoader,
    valid_dl: DataLoader,
    optimizer: torch.optim.Optimizer,
    n_epochs: int,
    device: torch.device,
    modelName,
    scheduler=None,
):
    """
    The main training loop which will optimize a given model on a given dataset
    :param model: The model being optimized
    :param train_dl: The training dataset
    :param valid_dl: A validation dataset
    :param optimizer: The optimizer used to update the model parameters
    :param n_epochs: Number of epochs to train for
    :param device: The device to train on
    :return: (model, losses) The best model and the losses per iteration
    """

    # Keep track of the loss and best accuracy
    losses = []
    learning_rates = []
    best_f1 = 0.0

    # Iterate through epochs
    for ep in range(n_epochs):

        model.train()
        loss_epoch = []
        appendToLogFile(" ")
        printAndLog(f"Model: {modelName}; Epoch {ep+1}/{n_epochs}")
        appendToLogFile("-" * 10)
        # Iterate through each batch in the dataloader
        for batch in tqdm(train_dl):

            # VERY IMPORTANT: Make sure the model is in training mode, which turns on
            # things like dropout and layer normalization

            # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch
            # keeps track of these dynamically in its computation graph so you need to explicitly
            # zero them out
            optimizer.zero_grad()

            # Place each tensor on the GPU
            batch = tuple(t.to(device) for t in batch)
            input_ids = batch[0]
            seq_lens = batch[1]
            labels = batch[2]

            # Pass the inputs through the model, get the current loss and logits
            logits, lengths, loss = model(input_ids, seq_lens, labels=labels)
            losses.append(loss.item())
            loss_epoch.append(loss.item())

            # Calculate all of the gradients and weight updates for the model
            loss.backward()

            # Optional: clip gradients
            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Finally, update the weights of the model
            optimizer.step()
            if scheduler != None:
                scheduler.step()
                learning_rates.append(scheduler.get_last_lr()[0])

        # gc.collect()

        # Perform inline evaluation at the end of the epoch
        f1 = evalBasic(model, valid_dl)
        print(f"Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}")
        appendToLogFile(
            f"Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}"
        )

        # Keep track of the best model based on the accuracy
        if f1 > best_f1:
            torch.save(model.state_dict(), modelName)
            best_f1 = f1
            # gc.collect()

    return losses, learning_rates


"""### Create an Encoder-Decoder Architecture with Beam Search"""


class EncoderBiLSTM(nn.Module):
    """Basic BiLSTM-CRF network for Lab 5"""

    def __init__(
        self,
        pretrained_embeddings: torch.tensor,
        lstm_dim: int,
        dropout_prob: float = 0.1,
    ):
        """
        Initializer for basic BiLSTM network
        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings
        :param lstm_dim: The dimensionality of the BiLSTM network
        :param dropout_prob: Dropout probability
        :param n_classes: The number of output classes
        """

        # First thing is to call the superclass initializer
        super(EncoderBiLSTM, self).__init__()

        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer
        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer
        self.model = nn.ModuleDict(
            {
                "embeddings": nn.Embedding.from_pretrained(
                    pretrained_embeddings, padding_idx=3
                ),
                "bilstm": nn.LSTM(
                    pretrained_embeddings.shape[1],  # input size
                    lstm_dim,  # hidden size
                    2,  # number of layers
                    batch_first=True,
                    bidirectional=True,
                ),
            }
        )
        self._init_weights()

    def _init_weights(self):
        all_params = list(self.model["bilstm"].named_parameters())
        for n, p in all_params:
            if "weight" in n:
                nn.init.xavier_normal_(p)
            elif "bias" in n:
                nn.init.zeros_(p)

    def forward(self, inputs, input_lens, hidden_states=None, labels=None):
        """
        Defines how tensors flow through the model
        :param inputs: (b x sl) The IDs into the vocabulary of the input samples
        :param input_lens: (b) The length of each input sequence
        :param labels: (b) The label of each sample
        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)
        """

        # Get embeddings (b x sl x edim)
        embeds = self.model["embeddings"](inputs)

        # Pack padded: This is necessary for padded batches input to an RNN - https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch
        lstm_in = nn.utils.rnn.pack_padded_sequence(
            embeds, input_lens.cpu(), batch_first=True, enforce_sorted=False
        )

        # Pass the packed sequence through the BiLSTM
        lstm_out, hidden_states = self.model["bilstm"](lstm_in)

        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)
        lstm_out, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)
        return lstm_out, hidden_states


class DecoderBiLSTM(nn.Module):
    """Basic BiLSTM-CRF network for Lab 5"""

    def __init__(
        self, pretrained_embeddings: torch.tensor, lstm_dim: int, n_classes: int = 2
    ):
        """
        Initializer for basic BiLSTM network
        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings
        :param lstm_dim: The dimensionality of the BiLSTM network
        :param dropout_prob: Dropout probability
        :param n_classes: The number of output classes
        """

        # First thing is to call the superclass initializer
        super(DecoderBiLSTM, self).__init__()

        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer
        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer
        self.model = nn.ModuleDict(
            {
                "embeddings": nn.Embedding.from_pretrained(
                    pretrained_embeddings, padding_idx=3
                ),
                "bilstm": nn.LSTM(
                    pretrained_embeddings.shape[1],  # input size
                    lstm_dim,  # hidden size
                    2,  # number of layers
                    batch_first=True,
                    bidirectional=True,
                ),
                "nn": nn.Linear(2 * lstm_dim, n_classes),
            }
        )
        self._init_weights()

    def _init_weights(self):
        all_params = list(self.model["bilstm"].named_parameters()) + list(
            self.model["nn"].named_parameters()
        )
        for n, p in all_params:
            if "weight" in n:
                nn.init.xavier_normal_(p)
            elif "bias" in n:
                nn.init.zeros_(p)

    def forward(self, inputs, hidden, input_lens):
        """
        Defines how tensors flow through the model
        :param inputs: (b x sl) The IDs into the vocabulary of the input samples
        :param hidden: (b) The hidden state of the previous step
        :param input_lens: (b) The length of each input sequence
        :return: (output predictions, lstm hidden states) the hidden states will be used as input at the next step
        """

        # Get embeddings (b x sl x edim)
        embeds = self.model["embeddings"](inputs)

        # Pack padded: This is necessary for padded batches input to an RNN - https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch
        lstm_in = nn.utils.rnn.pack_padded_sequence(
            embeds, input_lens.cpu(), batch_first=True, enforce_sorted=False
        )

        # Pass the packed sequence through the BiLSTM
        lstm_out, hidden_states = self.model["bilstm"](lstm_in, hidden)

        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)
        lstm_out, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)
        output = self.model["nn"](lstm_out)
        return output, hidden_states


class EncoderDecoderBiLSTM(nn.Module):
    """Basic Seq2Seq network"""

    def __init__(
        self,
        pretrained_embeddings: torch.tensor,
        lstm_dim: int,
        dropout_prob: float = 0.1,
        n_classes: int = 2,
        classWeights=None,
    ):
        """
        Initializer for basic Seq2Seq network
        :param pretrained_embeddings: A tensor containing the pretrained embeddings
        :param lstm_dim: The dimensionality of the LSTM network
        :param dropout_prob: Dropout probability
        :param n_classes: The number of output classes
        """

        # First thing is to call the superclass initializer
        super(EncoderDecoderBiLSTM, self).__init__()

        # We'll define the network in a ModuleDict, which consists of an encoder and a decoder
        self.model = nn.ModuleDict(
            {
                "encoder": EncoderBiLSTM(pretrained_embeddings, lstm_dim),
                "decoder": DecoderBiLSTM(pretrained_embeddings, lstm_dim, n_classes),
            }
        )
        self.labelNames = getLabels()
        # extract all keys in a list
        self.labelNames = list(self.labelNames.keys())

        # Calculate correct cross entropy class weights for imbalanced data
        appendToLogFile(f"Class Weights {classWeights}")
        if classWeights is not None:
            self.loss = nn.CrossEntropyLoss(weight=torch.FloatTensor(classWeights))
        else:
            self.loss = nn.CrossEntropyLoss()

    def forward(self, inputs, input_lens, labels=None):
        """
        Defines how tensors flow through the model.
        For the Seq2Seq model this includes 1) encoding the whole input text,
        and running *target_length* decoding steps to predict the tag of each token.

        :param inputs: (b x sl) The IDs into the vocabulary of the input samples
        :param input_lens: (b) The length of each input sequence
        :param labels: (b) The label of each sample
        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)
        """

        # Get embeddings (b x sl x embedding dim)
        encoder_output, encoder_hidden = self.model["encoder"](inputs, input_lens)
        decoder_hidden = encoder_hidden
        decoder_input = torch.tensor(
            [fastTxtTkn.encode(["[BOS]"])] * inputs.shape[0], device=device
        )
        target_length = labels.size(1)

        loss = None
        for di in range(target_length):
            decoder_output, decoder_hidden = self.model["decoder"](
                decoder_input, decoder_hidden, torch.tensor([1] * inputs.shape[0])
            )

            lossLabels = labels[:, di]
            decOut = decoder_output.squeeze(1)
            if loss == None:
                loss = self.loss(decOut, lossLabels)
            else:
                loss += self.loss(decOut, lossLabels)
            # Teacher forcing: Feed the target as the next input
            decoder_input = lossLabels.unsqueeze(-1)

        return loss / target_length


def trainBeam(
    model: nn.Module,
    train_dl: DataLoader,
    valid_dl: DataLoader,
    optimizer: torch.optim.Optimizer,
    n_epochs: int,
    device: torch.device,
    modelName,
    scheduler,
):
    """
    The main training loop which will optimize a given model on a given dataset
    :param model: The model being optimized
    :param train_dl: The training dataset
    :param valid_dl: A validation dataset
    :param optimizer: The optimizer used to update the model parameters
    :param n_epochs: Number of epochs to train for
    :param device: The device to train on
    :return: (model, losses) The best model and the losses per iteration
    """

    # Keep track of the loss and best accuracy
    losses = []
    best_f1 = 0.0
    learning_rates = []

    # Iterate through epochs
    for ep in range(n_epochs):

        model.train()
        loss_epoch = []
        appendToLogFile(" ")
        printAndLog(f"Model: {modelName}; Epoch {ep+1}/{n_epochs}")
        appendToLogFile("-" * 10)

        # Iterate through each batch in the dataloader
        for batch in tqdm(train_dl):
            # VERY IMPORTANT: Make sure the model is in training mode, which turns on
            # things like dropout and layer normalization

            # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch
            # keeps track of these dynamically in its computation graph so you need to explicitly
            # zero them out
            optimizer.zero_grad()

            # Place each tensor on the GPU
            batch = tuple(t.to(device) for t in batch)
            input_ids = batch[0]
            input_lens = batch[1]
            labels = batch[2]

            # Pass the inputs through the model, get the current loss and logits
            loss = model(input_ids, labels=labels, input_lens=input_lens)
            losses.append(loss.item())
            loss_epoch.append(loss.item())

            # Calculate all of the gradients and weight updates for the model
            loss.backward()

            # Optional: clip gradients
            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Finally, update the weights of the model
            optimizer.step()
            if scheduler != None:
                scheduler.step()
                learning_rates.append(scheduler.get_last_lr()[0])

        # Perform inline evaluation at the end of the epoch
        f1 = evalBeam(model, valid_dl)
        print(f"Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}")
        appendToLogFile(
            f"Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}"
        )

        # Keep track of the best model based on the accuracy
        if f1 > best_f1:
            torch.save(model.state_dict(), modelName)
            best_f1 = f1

    return losses


softmax = nn.Softmax(dim=-1)


def decode(
    model, inputs: torch.Tensor, input_lens, logPrediction, labels=None, beam_size=2
):
    """
    Decoding/predicting the labels for an input text by running beam search.

    :param inputs: (b x sl) The IDs into the vocabulary of the input samples
    :param input_lens: (b) The length of each input sequence
    :param labels: (b) The label of each sample
    :param beam_size: the size of the beam
    :return: predicted sequence of labels
    """
    assert inputs.shape[0] == 1
    # first, encode the input text
    encoder_output, encoder_hidden = model.model["encoder"](inputs, input_lens)
    decoder_hidden = encoder_hidden

    # the decoder starts generating after the Begining of Sentence (BOS) token
    decoder_input = torch.tensor(
        [
            fastTxtTkn.encode(
                [
                    "[BOS]",
                ]
            ),
        ],
        device=device,
    )
    target_length = labels.shape[1]

    # we will use heapq to keep top best sequences so far sorted in heap_queue
    # these will be sorted by the first item in the tuple
    heap_queue = []
    heap_queue.append(
        (torch.tensor(0), fastTxtTkn.encode(["[BOS]"]), decoder_input, decoder_hidden)
    )

    # Beam Decoding
    for tlIdx in range(target_length):
        # print("next len")
        new_items = []
        # for each item on the beam
        for j in range(len(heap_queue)):
            # 1. remove from heap
            score, tokens, decoder_input, decoder_hidden = heapq.heappop(heap_queue)
            # 2. decode one more step
            decoder_output, decoder_hidden = model.model["decoder"](
                decoder_input, decoder_hidden, torch.tensor([1])
            )
            decoder_output = softmax(decoder_output)
            # 3. get top-k predictions
            best_idx = torch.argsort(decoder_output[0], descending=True)[0]

            for i in range(beam_size):
                decoder_input = torch.tensor([[best_idx[i]]], device=device)

                new_items.append(
                    (
                        score + decoder_output[0, 0, best_idx[i]],
                        tokens + [best_idx[i].item()],
                        decoder_input,
                        decoder_hidden,
                    )
                )

            best_idx_text = " ".join(
                [fastTxtTkn.decode(i) for i in best_idx.cpu().numpy()]
            )
            # extract the numerical values from decoder_output
            decOut = decoder_output[0, 0].cpu().detach().numpy()
            expLabel = idxToLabel(labels[0, tlIdx].cpu().detach().item())

            # Logging..
            # ans = [fastTxtTkn.decode(i) for i in best_idx.cpu().numpy() if i != getLabels()['O']]
            # expAns = [fastTxtTkn.decode(i) for i in expLabel if i != getLabels()['O']]
            # if cnt % 100 == 0:
            #     appendToLogFile(f'best_idx: {best_idx_text}, decOut: {ans}, exp {expLabel}')

        # add new sequences to the heap
        for item in new_items:
            # print(item)
            heapq.heappush(heap_queue, item)
        # remove sequences with lowest score (items are sorted in descending order)
        while len(heap_queue) > beam_size:
            heapq.heappop(heap_queue)

    final_sequence = heapq.nlargest(1, heap_queue)[0]

    if logPrediction:
        inputTxt, predAns, gt = getPrompt_Pred_Gt(inputs, final_sequence, labels)
        appendToLogFile(f"Prmt: {inputTxt}")
        appendToLogFile(f"Pred: {predAns}")
        appendToLogFile(f"GrTh: {gt} \n\n")

    assert labels.shape[1] == len(final_sequence[1][1:])
    return final_sequence

def getPrompt_Pred_Gt(inputs, final_sequence, labels):
    # Input text
    inputIds = inputs.cpu().numpy()[0]
    input_texts = " ".join([fastTxtTkn.decode(x) for x in inputIds])
    input_texts = "\n".join(
        input_texts[i : i + 80] for i in range(0, len(input_texts), 80)
    )

    # Prediction
    predAns = ""
    for index, entry in enumerate(final_sequence[1][1:]):
        if entry == 1 or entry == 2:
            predAns += fastTxtTkn.decode(inputIds[index]) + f"({idxToLabel(entry)}) "
    if predAns == "":
        predAns = "Not answerable!"

    # Ground truth
    groundTruth = ""
    labelNumpy = labels.cpu().numpy()
    for index, entry in enumerate(labelNumpy[0]):
        if entry == 1 or entry == 2:
            groundTruth += (
                fastTxtTkn.decode(inputIds[index]) + f"({idxToLabel(entry)}) "
            )
    if groundTruth == "":
        groundTruth = "Not answerable!"
    return input_texts, predAns, groundTruth



def evalBeam(model: nn.Module, valid_dl: DataLoader, beam_size: int = 1):
    """
    Evaluates the model on the given dataset
    :param model: The model under evaluation
    :param valid_dl: A `DataLoader` reading validation data
    :return: The accuracy of the model on the dataset
    """
    # VERY IMPORTANT: Put your model in "eval" mode -- this disables things like
    # layer normalization and dropout
    model.eval()
    labels_all = []
    logits_all = []
    tags_all = []
    logPredictionModulus = int(len(valid_dl) / 4)  # log every 4th batch

    # ALSO IMPORTANT: Don't accumulate gradients during this process
    with torch.no_grad():
        for idx, batch in enumerate(tqdm(valid_dl, desc="Evaluation")):
            batch = tuple(t.to(device) for t in batch)
            input_ids = batch[0]
            input_lens = batch[1]
            labels = batch[2]

            # Convert input_ids to a numpy array and then to text using tokenizer
            if idx % logPredictionModulus == 0:
                for id, lbl in zip(input_ids, labels):
                    input_texts = " ".join(
                        [fastTxtTkn.decode(x) for x in id.cpu().numpy()]
                    )
                    if DEBUG:
                        print(f"IDs Len: {len(id.cpu().numpy())}; {input_texts}")
                    labels_texts = " ".join(
                        [fastTxtTkn.decode(x) for x in lbl.cpu().numpy()]
                    )
                    if DEBUG:
                        print(f"lbl Len: {len(lbl.cpu().numpy())}; {labels_texts}")

            logPrediction = idx % logPredictionModulus == 0
            best_seq = decode(
                model,
                input_ids,
                input_lens,
                logPrediction,
                labels=labels,
                beam_size=beam_size,
            )
            mask = input_ids != 0

            # Expanded for logging
            labelsAndInput = zip(list(labels.detach().cpu().numpy()), input_ids)
            for seq, samp in labelsAndInput:
                txt = ""
                for l, i in zip(seq, samp):
                    if i != 0:
                        txt += fastTxtTkn.decode(l)
                        # Convert L to text
                        labels_all.append(l)
                if DEBUG:
                    print(f"l Len: {len(seq)}; {txt}")

            bs = best_seq[1][1:]
            bs_txt = " ".join([fastTxtTkn.decode(x) for x in bs])
            tags_all += bs

            # Debugging
            if DEBUG:
                print(f"Len BS: {len(bs)}; {bs_txt}")
            # print(best_seq[1][1:], labels)

    if len(tags_all) != len(labels_all):
        # force the length to be the same
        tags_all = tags_all[: len(labels_all)]
        print(f"Tags did not match labels. Truncated tags to {len(tags_all)}")
    P, R, F1, _ = precision_recall_fscore_support(labels_all, tags_all, average="macro")
    print(confusion_matrix(labels_all, tags_all))
    appendToLogFile(f"F1: {F1}")
    appendToLogFile(f"Confusion matrix:")
    appendToLogFile(f"{np.array2string(confusion_matrix(labels_all, tags_all))}")
    printAndLog(classification_report(labels_all, tags_all))
    return F1


"""### Start training

"""
# Load and parse the dataset
ds = loadTyDiQaDataset()

# Training hyperparameters
languages = ["english", "finnish", "japanese"]
device = (torch.device("cpu"), torch.device("cuda"))[torch.cuda.is_available()]
lstm_dim = 300
dropout_prob = 0.25
batch_size = 256
lr = 0.001
n_epochs = 75
n_workers = 0
cwDivisors = [(1, 1), (0.9, 0.9), (0.75, 0.85)]

for language in languages:
    printAndLog("\n\n" + ("-" * 50))
    printAndLog(f"Language: {language}")
    trainSet, c_weights = parseData(ds, language, "train")
    valSet, _ = parseData(ds, language, "validation")

    # Tokenize
    vocabTokens = extractVocabTokens(trainSet, valSet)
    vocabulary, pretrainedEmb = load_vectors(vocabTokens)
    fastTxtTkn = FasttextTokenizer(vocabulary)

    trainDl = getDataLoader(trainSet, batch_size, True)
    valDl = getDataLoader(valSet, 1, False)
    appendToLogFile(f"Class weights {c_weights}")

    for cwDivisor in cwDivisors:
        torch.cuda.empty_cache()
        appendToLogFile(f"Starting training with CW divisor: {cwDivisor}")

        c_weights = [c_weights[0], c_weights[1] * cwDivisor[0], c_weights[2] * cwDivisor[1]]
        appendToLogFile(f"New class weights {c_weights}")

        modelName = f"lab5_basicLSTM_model_cw{cwDivisor}.pt"
        model = BiLSTM(
            pretrained_embeddings=torch.FloatTensor(pretrainedEmb),
            lstm_dim=lstm_dim,
            dropout_prob=dropout_prob,
            n_classes=len(getLabels()),
            classWeights=c_weights,
        ).to(device)

        # Create the optimizer
        optimizer = AdamW(model.parameters(), lr=lr)
        scheduler = CyclicLR(
            optimizer,
            base_lr=0.0,
            max_lr=lr,
            step_size_up=1,
            step_size_down=(len(trainDl) * n_epochs),
            cycle_momentum=False,
        )

        # Train
        appendToLogFile("Starting training of basic model ")
        appendToLogFile(
            f"Epochs {n_epochs} epochs;\n \
                            batch size {batch_size};\n \
                            learning rate {lr}; \n \
                            lstm dim {lstm_dim};\n \
                            dropout {dropout_prob}"
        )
        losses, learning_rates = trainBasic(
            model, trainDl, valDl, optimizer, n_epochs, device, modelName, scheduler
        )
        model.load_state_dict(torch.load(modelName))
        appendToLogFile("Finished training of basic model")

        appendToLogFile("Starting evaluation of basic model")
        # Evaluate
        evalBasic(model, valDl)
        appendToLogFile("Finished evaluation of basic model")
        appendToLogFile(" ")
        appendToLogFile(" ")
        appendToLogFile(" ")

        """### Train the Encoder-Decoder with Beam Search"""

        modelName = f"lab5_beamSearch_model_cw{cwDivisor}.pt"
        # Create the model
        model = EncoderDecoderBiLSTM(
            pretrained_embeddings=torch.FloatTensor(pretrainedEmb),
            lstm_dim=lstm_dim,
            dropout_prob=dropout_prob,
            n_classes=len(getLabels()),
            classWeights=c_weights,
        ).to(device)

        # Create the optimizer
        optimizer = AdamW(model.parameters(), lr=lr)
        scheduler = CyclicLR(
            optimizer,
            base_lr=0.0,
            max_lr=lr,
            step_size_up=1,
            step_size_down=(len(trainDl) * n_epochs),
            cycle_momentum=False,
        )

        # Train
        appendToLogFile(
            f"Training beam search model with {n_epochs} epochs; batch size {batch_size}; learning rate {lr}"
        )
        losses = trainBeam(
            model, trainDl, valDl, optimizer, n_epochs, device, modelName, scheduler
        )
        model.load_state_dict(torch.load(modelName))
        appendToLogFile(f"Training done")
        appendToLogFile(f"")

        appendToLogFile(f"Evaluating with beam size 1")
        evalBeam(model, valDl, beam_size=1)
        appendToLogFile(f"")
        appendToLogFile(f"Evaluating with beam size 2")
        evalBeam(model, valDl, beam_size=2)
        appendToLogFile(f"Finished successfully!")
